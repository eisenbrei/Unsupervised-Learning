---
title: "Wine attribute clustering"
author: "Matt Eisenbrei"
date: "2/29/2020"
output:
  html_document: default
  pdf_document: default
---

## Analysis goal: determine whether a data set of 178 wines can be clustered based on measurements from 13 quantitative variables.

### This analysis is an example of simple clustering, the principles of which can be applied to more complex clustering analyses.

I will use data from the "rattle" package which includes a wine data set, originally posted to UCI in 1991.
```{r}
setwd("E:/Matt/Analyses")
library(rattle)
wine=as.data.frame(wine)
str(wine)
summary(wine)
```

There are 178 observations of 14 variables in the original data set.

I will clean the data to the extent required, first by checking for missing (NA) values:
```{r}
sapply(wine,function(x)sum(is.na(x))) 
```

Fortunately, there was no missing data.

Next, I will remove the "Type" variable from the data set to keep only numerical (non-factor) variables for the initial analysis.
```{r}
wine.data=wine[,-1]
str(wine.data)
```

Now, I'll check for correlation between variables and conduct some exploratory data analysis.
```{r}
library(corrplot)
corrplot(cor(wine.data),method="number")
```

Flavanoids and Phenols have the highest correlation at .86 which will likely affect the cluster identities.

Box plots can be useful in better understanding scale and variation by variable.
```{r}
par(mfrow=c(3,5))
boxplot(wine.data$Alcohol,main="Alcohol",col="cadetblue1")
boxplot(wine.data$Malic,main="Malic",col="cadetblue4")
boxplot(wine.data$Ash,main="Ash",col="chartreuse1")
boxplot(wine.data$Alcalinity,main="Alcalinity",col="chartreuse4")
boxplot(wine.data$Magnesium,main="Magnesium",col="chocolate1")
boxplot(wine.data$Phenols,main="Phenols",col="chocolate4")
boxplot(wine.data$Flavanoids,main="Flavanoids",col="coral1")
boxplot(wine.data$Nonflavanoids,main="Nonflavanoids",col="coral4")
boxplot(wine.data$Proanthocyanins,main="Proanthocyanins",col="blue1")
boxplot(wine.data$Color,main="Color",col="blue4")
boxplot(wine.data$Hue,main="Hue",col="darkgoldenrod1")
boxplot(wine.data$Dilution,main="Dilution",col="darkgoldenrod4")
boxplot(wine.data$Proline,main="Proline (Note the scale)",col="darkgrey")
par(mfrow=c(1,1))
```

Looking at the boxplots, "Proline" and "Magnesium" use a very different scale from the other variables.

That's sufficient for the general data exploration. Now it's time to use principal components analysis (PCA) for initial cluster visualization.

PCA:
```{r}
wine.data=scale(wine.data)
pr.out=prcomp(wine.data)
summary(pr.out)
```

It appears that two principal components can explain 55% of the variance, with the first component alone explaining 36%.

```{r}
pr.out$rotation
```

PC1 wines have a strong negative relationship with Phenols and Flavanoids. I'll explore the attributes further once clusters have been identified.

Now I'll prepare biplots, examine the proportion of variance explained by principal components, and create a scree plot to estimate the optimal number of clusters.

Starting with bi-plots, which show the eigenvalues of attributes as a representation of their strength in determining clusters.
```{r}
biplot(pr.out, scale=0)
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)
pr.var=pr.out$sdev^2
pr.var
```

Now, I will create a scree plot to help identify the optimal number of clusters.
```{r}
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
```

Looking at the plot, the bend (or "elbow") appears at three clusters. However, assessing the true number of clusters from scree plots is an art more than a science, so I will use other methods as well.

Now to assess whether true clusters exist:
```{r}
library(factoextra)
library(clustertend)
res=get_clust_tendency(wine.data,n=nrow(wine.data)-1,graph=FALSE) 
res$hopkins_stat
```

The Hopkins statistic is .72 which indicates that meaningful clusters likely exist in the data since it is close to 1.

Now I will visually inspect the data for clustering.
```{r}
fviz_pca_ind(prcomp(wine.data),title="PCA - wine data", habillage = wine$Type, palette = "jco", geom = "point", ggtheme = theme_classic(), legend = "bottom")
```

It looks like 3 fairly distinct clusters.

Now I'll assess the optimal number of clusters using three different methods: elbow, silhouette, and the gap statistic.
```{r}
library(NbClust)
```

First, the elbow method:
```{r}
fviz_nbclust(wine.data,kmeans,method="wss") + geom_vline(xintercept=4,linetype=2) + labs(subtitle="Elbow method")
```

The elbow method suggests that there are four optimal clusters.

Next, the silhouette method:
```{r}
fviz_nbclust(wine.data,kmeans,method="silhouette") + labs(subtitle="Silhouette method")
```

The silhouette method suggests three optimal clusters.

Last, the gap statistic:
```{r}
set.seed(555) 
fviz_nbclust(wine.data,kmeans,nstart=25,method="gap_stat",nboot=100) + labs(subtitle="Gap statistic method")
```

The gap statistic also suggests three optimal clusters.

Two of the three methods suggested 3 clusters, so we'll start our assumption with 3 but also test for 4 clusters.

clValid is another useful tool to help assess not only the optimal number of clusters, but the best clustering methodology.
```{r}
library(clValid)
clmethods=c("hierarchical","kmeans","pam")
intern=clValid(wine.data,nClust=2:4,clMethods=clmethods,validation="internal") 
summary(intern)
```

clValid suggests a hierarchical approach with 2 clusters or k-means with 3 clusters. 

The PCA plot indicated 3 clear clusters, so k-means with 3 clusters will be the working assumption.

Next, I'll actually execute K-means clustering. I will check solutions with both 3 and 4 clusters.

3 cluster k-means:
```{r}
set.seed(27)
km.3clus=kmeans(wine.data,3,nstart=50)
names(km.3clus)
km.3clus$tot.withinss
km.3clus$cluster
km.3clus$centers
clus.plot3=fviz_cluster(km.3clus, geom = "point", data =wine.data) + ggtitle("K-means with k = 3")
clus.plot3
```

4 cluster k-means:
```{r}
set.seed(27)
km.4clus=kmeans(wine.data,4,nstart=50)
km.4clus$tot.withinss
km.4clus$cluster
km.4clus$centers
clus.plot4=fviz_cluster(km.4clus, geom = "point", data =wine.data) + ggtitle("K-means with k = 4")
clus.plot4
```

Looking at the plots, the 3-cluster solution is clearly better than the 4-cluster solution, even though the total within-cluster sum of squares value is lower (as a representation of compactness) in the four cluster model.

Next, I will add the cluster identities back into the data set to better understand which variable values differentiate the clusters.
```{r}
library(dplyr)
product.groups=(km.3clus$cluster)
product.groups=as.data.frame(product.groups)
wine.data=as.data.frame(wine.data)
wine.data.clustered=bind_cols(wine.data[,1:13],product.groups)
str(wine.data.clustered)
```

Now to rename the customer.segments variable and change it to an integer.
```{r}
names(wine.data.clustered)[14]="Product_Group"
wine.data.clustered$Product_Group=as.factor(wine.data.clustered$Product_Group)
str(wine.data.clustered)
```

Here are the observations belonging to each group...
```{r}
wine.data.clustered.seg1=subset(wine.data.clustered,Product_Group==1)
wine.data.clustered.seg2=subset(wine.data.clustered,Product_Group==2)
wine.data.clustered.seg3=subset(wine.data.clustered,Product_Group==3)
```

And a quick view of some of the observations in Product_Group 1:

```{r}
head(wine.data.clustered.seg1)
```

Now I will reshape the data to set the product groupings as rows.
```{r}
library(reshape)
segment.all=melt(wine.data.clustered,id="Product_Group")
segment.all.cast=cast(segment.all,Product_Group~variable,mean)
segment.all.cast
```

Next I update the cluster names.
```{r}
segment.all.cast$Product_Group=gsub("1","Product Group 1",segment.all.cast$Product_Group)
segment.all.cast$Product_Group=gsub("2","Product Group 2",segment.all.cast$Product_Group)
segment.all.cast$Product_Group=gsub("3","Product Group 3",segment.all.cast$Product_Group)
segment.all.cast
```

Then I convert the Product_Group variable to a row name.
```{r}
segment.all.cast.1=segment.all.cast[,-1]
rownames(segment.all.cast.1)=segment.all.cast[,1]
rownames(segment.all.cast.1)
```

Now, I'll plot the data using radar charts which visually show differentiating factors by cluster.

Radar plots cannot handle negative values so the absolute value of the minimum will be added to all data points in the set to scale the data for the plot type.
```{r}
min.value=min(segment.all.cast.1)
min.value
```

The minimum value is -1.288776. The absolute value of the minimum will be added to all observations in the data set to re-set the new minimum value to 0 for the radar plot.
```{r}
data.for.radar=segment.all.cast.1+(abs(min.value))
data.for.radar
```

Now here is the radar plot:
```{r}
library(fmsb)
set.seed(50)
max.seg=max(data.for.radar)
min.seg=min(data.for.radar)
```

I have to add rows representing maximum and minimum values to the data set for the purpose of creating the chart.
```{r}
segment.radar=rbind(rep(max.seg,13),rep(min.seg,13),data.for.radar)
```

Now I'll create the plot:
```{r}
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )

radarchart(segment.radar,axistype=1,pcol=colors_border,pfcol=colors_in,plwd=4,plty=1,cglcol="grey",cglty=1,axislabcol="grey",caxislabels=seq(min.seg,max.seg,13),cglwd=0.8,vlcex=0.9)

legend(x=1.1, y=1.3, legend = rownames(segment.radar[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.0, pt.cex=3)
```

Using the radar chart, it is easy to understand the differences between the clusters.

Product Group 1:
Product Group 1 has comparatively moderate values across the board but is slightly low on "Malic" and "Magnesium." These wines have slightly higher values for "Hue" and "Alcalinity."

Product Group 2:
Product Group 2 wines are differentiated by very high values for "Alcohol", "Proline", "Magnesium", "Phenols", and "Flavanoids." These wines have comparatively lower values for "Nonflavanoids" and "Alcalinity."

Product Group 3:
Product Group 3 wines have comparatively high values for "Color", "Nonflavanoids", "Alcalinity", and "Malic". These wines have higher lower values for "Dilution", "Hue", and "Flavanoids."








